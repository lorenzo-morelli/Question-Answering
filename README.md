# Question Answering System based on the Stanford QUestion-Answering Dataset

In this Python Jupyter Notebook project, we explored the fascinating world of Question Answering in Natural Language Processing. Using the [Stanford SQuAD2.0](https://rajpurkar.github.io/SQuAD-explorer/) dataset, specially designed for this task, we trained three powerful Transformer-based models: Google's [BERT](https://huggingface.co/bert-base-uncased), Facebook AI's [RoBERTa](https://huggingface.co/roberta-base), and Hugging Face's (DistilBERT[https://huggingface.co/distilbert-base-uncased]). The dataset contains around 150,000 questions collected from Wikipedia articles, with answers provided either as text spans or marked as unanswerable. Our goal was to build models that not only accurately answer questions when possible but also abstain from answering when a question is unanswerable.

We started with Exploratory Data Analysis, visualizing the length distribution of questions and answers, and observing the distribution of answer types. We also attempted clustering on context texts to identify any patterns. Next, we evaluated the models using the SQuAD evaluation script, comparing both base and fine-tuned versions of the models. Fine-tuning significantly improved the F1 score, with RoBERTa emerging as the best-performing model.

To make the project interactive, we provided a function to test the models with custom questions and contexts. Additionally, we integrated Wikipedia's API to retrieve the context for a given question, enabling the models to find and answer questions from Wikipedia pages. This added real-world applicability to the models, making them a powerful tool for extracting information from vast textual data sources like Wikipedia. Overall, the project showcased the potential of state-of-the-art Transformer-based models in the exciting field of Question Answering.